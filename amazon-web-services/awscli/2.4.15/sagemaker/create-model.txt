CREATE-MODEL()                                                  CREATE-MODEL()



NAME
       create-model -

DESCRIPTION
       Creates a model in Amazon SageMaker. In the request, you name the model
       and describe a primary container. For the primary container, you  spec-
       ify  the  Docker  image  that  contains inference code, artifacts (from
       prior training), and a custom environment map that the  inference  code
       uses when you deploy the model for predictions.

       Use  this  API  to  create  a model if you want to use Amazon SageMaker
       hosting services or run a batch transform job.

       To host your model, you create an endpoint configuration with the  Cre-
       ateEndpointConfig  API, and then create an endpoint with the CreateEnd-
       point API. Amazon SageMaker then deploys all of the containers that you
       defined for the model in the hosting environment.

       For  an example that calls this method when deploying a model to Amazon
       SageMaker hosting services, see Deploy the Model  to  Amazon  SageMaker
       Hosting Services (Amazon Web Services SDK for Python (Boto 3)).

       To  run  a  batch  transform using your model, you start a job with the
       CreateTransformJob API. Amazon  SageMaker  uses  your  model  and  your
       dataset  to get inferences which are then saved to a specified S3 loca-
       tion.

       In the CreateModel request, you must define a container with the Prima-
       ryContainer parameter.

       In  the request, you also provide an IAM role that Amazon SageMaker can
       assume to access model artifacts and docker image for deployment on  ML
       compute hosting instances or for batch transform jobs. In addition, you
       also use the IAM role to manage permissions the inference  code  needs.
       For example, if the inference code access any other Amazon Web Services
       resources, you grant necessary permissions via this role.

       See also: AWS API Documentation

       See 'aws help' for descriptions of global parameters.

SYNOPSIS
            create-model
          --model-name <value>
          [--primary-container <value>]
          [--containers <value>]
          [--inference-execution-config <value>]
          --execution-role-arn <value>
          [--tags <value>]
          [--vpc-config <value>]
          [--enable-network-isolation | --no-enable-network-isolation]
          [--cli-input-json | --cli-input-yaml]
          [--generate-cli-skeleton <value>]

OPTIONS
       --model-name (string)
          The name of the new model.

       --primary-container (structure)
          The location of the primary docker image containing inference  code,
          associated  artifacts, and custom environment map that the inference
          code uses when the model is deployed for predictions.

          ContainerHostname -> (string)
              This parameter is ignored for models that contain only a  Prima-
              ryContainer .

              When a ContainerDefinition is part of an inference pipeline, the
              value of the parameter uniquely identifies the container for the
              purposes  of  logging and metrics. For information, see Use Logs
              and Metrics to Monitor an Inference  Pipeline  .  If  you  don't
              specify  a  value  for  this parameter for a ContainerDefinition
              that is part of an inference pipeline, a unique name is automat-
              ically assigned based on the position of the ContainerDefinition
              in the pipeline. If you specify a value for  the  ContainerHost-
              Name  for  any  ContainerDefinition that is part of an inference
              pipeline, you must specify a  value  for  the  ContainerHostName
              parameter of every ContainerDefinition in that pipeline.

          Image -> (string)
              The  path  where inference code is stored. This can be either in
              Amazon EC2 Container Registry or in a Docker  registry  that  is
              accessible  from  the  same VPC that you configure for your end-
              point. If you are using your own custom algorithm instead of  an
              algorithm  provided by Amazon SageMaker, the inference code must
              meet Amazon SageMaker requirements.  Amazon  SageMaker  supports
              both  registry/repository[:tag] and registry/repository[@digest]
              image path formats. For more information,  see  Using  Your  Own
              Algorithms with Amazon SageMaker

          ImageConfig -> (structure)
              Specifies whether the model container is in Amazon ECR or a pri-
              vate Docker registry accessible from your Amazon Virtual Private
              Cloud  (VPC). For information about storing containers in a pri-
              vate Docker registry, see Use  a  Private  Docker  Registry  for
              Real-Time Inference Containers

              RepositoryAccessMode -> (string)
                 Set this to one of the following values:

                 o Platform - The model image is hosted in Amazon ECR.

                 o Vpc  -  The  model image is hosted in a private Docker reg-
                   istry in your VPC.

              RepositoryAuthConfig -> (structure)
                 (Optional) Specifies an authentication configuration for  the
                 private  docker  registry  where  your model image is hosted.
                 Specify a value for this property only if you  specified  Vpc
                 as the value for the RepositoryAccessMode field, and the pri-
                 vate Docker registry where the model image is hosted requires
                 authentication.

                 RepositoryCredentialsProviderArn -> (string)
                     The  Amazon Resource Name (ARN) of an Amazon Web Services
                     Lambda function that provides credentials to authenticate
                     to  the private Docker registry where your model image is
                     hosted. For information about how to create an Amazon Web
                     Services  Lambda  function,  see Create a Lambda function
                     with the console in the Amazon Web Services Lambda Devel-
                     oper Guide .

          Mode -> (string)
              Whether the container hosts a single model or multiple models.

          ModelDataUrl -> (string)
              The  S3  path where the model artifacts, which result from model
              training, are stored. This path must point to a single gzip com-
              pressed  tar  archive  (.tar.gz suffix). The S3 path is required
              for Amazon SageMaker built-in algorithms, but  not  if  you  use
              your  own  algorithms.  For  more  information on built-in algo-
              rithms, see Common Parameters .

              NOTE:
                 The model artifacts must be in an S3 bucket that  is  in  the
                 same region as the model or endpoint you are creating.

              If you provide a value for this parameter, Amazon SageMaker uses
              Amazon Web Services Security Token  Service  to  download  model
              artifacts  from the S3 path you provide. Amazon Web Services STS
              is activated in your IAM user account by default. If you  previ-
              ously deactivated Amazon Web Services STS for a region, you need
              to reactivate Amazon Web Services STS for that region. For  more
              information, see Activating and Deactivating Amazon Web Services
              STS in an Amazon Web Services Region in the Amazon Web  Services
              Identity and Access Management User Guide .

              WARNING:
                 If  you  use  a  built-in algorithm to create a model, Amazon
                 SageMaker requires that you provide a S3 path  to  the  model
                 artifacts in ModelDataUrl .

          Environment -> (map)
              The  environment  variables to set in the Docker container. Each
              key and value in the Environment string to string map  can  have
              length of up to 1024. We support up to 16 entries in the map.

              key -> (string)

              value -> (string)

          ModelPackageName -> (string)
              The  name  or Amazon Resource Name (ARN) of the model package to
              use to create the model.

          InferenceSpecificationName -> (string)
              The inference specification name in the model package version.

          MultiModelConfig -> (structure)
              Specifies additional configuration for multi-model endpoints.

              ModelCacheSetting -> (string)
                 Whether to  cache  models  for  a  multi-model  endpoint.  By
                 default,  multi-model  endpoints cache models so that a model
                 does not have to be  loaded  into  memory  each  time  it  is
                 invoked.  Some  use  cases do not benefit from model caching.
                 For example, if an endpoint hosts a large  number  of  models
                 that  are  each invoked infrequently, the endpoint might per-
                 form better if you disable model caching.  To  disable  model
                 caching, set the value of this parameter to Disabled .

       Shorthand Syntax:

          ContainerHostname=string,Image=string,ImageConfig={RepositoryAccessMode=string,RepositoryAuthConfig={RepositoryCredentialsProviderArn=string}},Mode=string,ModelDataUrl=string,Environment={KeyName1=string,KeyName2=string},ModelPackageName=string,InferenceSpecificationName=string,MultiModelConfig={ModelCacheSetting=string}

       JSON Syntax:

          {
            "ContainerHostname": "string",
            "Image": "string",
            "ImageConfig": {
              "RepositoryAccessMode": "Platform"|"Vpc",
              "RepositoryAuthConfig": {
                "RepositoryCredentialsProviderArn": "string"
              }
            },
            "Mode": "SingleModel"|"MultiModel",
            "ModelDataUrl": "string",
            "Environment": {"string": "string"
              ...},
            "ModelPackageName": "string",
            "InferenceSpecificationName": "string",
            "MultiModelConfig": {
              "ModelCacheSetting": "Enabled"|"Disabled"
            }
          }

       --containers (list)
          Specifies the containers in the inference pipeline.

          (structure)
              Describes the container, as part of model definition.

              ContainerHostname -> (string)
                 This parameter is ignored for models that contain only a Pri-
                 maryContainer .

                 When a ContainerDefinition is part of an inference  pipeline,
                 the  value of the parameter uniquely identifies the container
                 for the purposes of logging and metrics. For information, see
                 Use  Logs  and  Metrics to Monitor an Inference Pipeline . If
                 you don't specify a value for this parameter for  a  Contain-
                 erDefinition  that is part of an inference pipeline, a unique
                 name is automatically assigned based on the position  of  the
                 ContainerDefinition  in  the pipeline. If you specify a value
                 for the ContainerHostName for any ContainerDefinition that is
                 part  of  an inference pipeline, you must specify a value for
                 the ContainerHostName parameter of every  ContainerDefinition
                 in that pipeline.

              Image -> (string)
                 The  path  where inference code is stored. This can be either
                 in Amazon EC2 Container Registry or in a Docker registry that
                 is  accessible  from the same VPC that you configure for your
                 endpoint. If you are using your own custom algorithm  instead
                 of  an  algorithm provided by Amazon SageMaker, the inference
                 code must meet Amazon SageMaker  requirements.  Amazon  Sage-
                 Maker   supports   both  registry/repository[:tag]  and  reg-
                 istry/repository[@digest] image path formats. For more infor-
                 mation, see Using Your Own Algorithms with Amazon SageMaker

              ImageConfig -> (structure)
                 Specifies  whether  the model container is in Amazon ECR or a
                 private Docker registry accessible from your  Amazon  Virtual
                 Private Cloud (VPC). For information about storing containers
                 in a private Docker registry, see Use a Private  Docker  Reg-
                 istry for Real-Time Inference Containers

                 RepositoryAccessMode -> (string)
                     Set this to one of the following values:

                     o Platform - The model image is hosted in Amazon ECR.

                     o Vpc  -  The  model  image is hosted in a private Docker
                       registry in your VPC.

                 RepositoryAuthConfig -> (structure)
                     (Optional) Specifies an authentication configuration  for
                     the  private  docker  registry  where your model image is
                     hosted. Specify a value for this  property  only  if  you
                     specified  Vpc  as the value for the RepositoryAccessMode
                     field, and the private Docker registry  where  the  model
                     image is hosted requires authentication.

                     RepositoryCredentialsProviderArn -> (string)
                        The  Amazon  Resource Name (ARN) of an Amazon Web Ser-
                        vices Lambda function  that  provides  credentials  to
                        authenticate to the private Docker registry where your
                        model image is hosted. For information  about  how  to
                        create  an  Amazon  Web  Services Lambda function, see
                        Create a Lambda function with the console in the  Ama-
                        zon Web Services Lambda Developer Guide .

              Mode -> (string)
                 Whether  the  container hosts a single model or multiple mod-
                 els.

              ModelDataUrl -> (string)
                 The S3 path where the  model  artifacts,  which  result  from
                 model  training, are stored. This path must point to a single
                 gzip compressed tar archive (.tar.gz suffix). The S3 path  is
                 required for Amazon SageMaker built-in algorithms, but not if
                 you use your own algorithms. For more information on built-in
                 algorithms, see Common Parameters .

                 NOTE:
                     The  model  artifacts  must be in an S3 bucket that is in
                     the same region as the model or endpoint you  are  creat-
                     ing.

                 If  you  provide a value for this parameter, Amazon SageMaker
                 uses Amazon Web Services Security Token Service  to  download
                 model artifacts from the S3 path you provide. Amazon Web Ser-
                 vices STS is activated in your IAM user account  by  default.
                 If  you  previously deactivated Amazon Web Services STS for a
                 region, you need to reactivate Amazon Web  Services  STS  for
                 that region. For more information, see Activating and Deacti-
                 vating Amazon Web Services STS  in  an  Amazon  Web  Services
                 Region in the Amazon Web Services Identity and Access Manage-
                 ment User Guide .

                 WARNING:
                     If you use a built-in algorithm to create a model, Amazon
                     SageMaker  requires  that  you  provide  a S3 path to the
                     model artifacts in ModelDataUrl .

              Environment -> (map)
                 The environment variables to set  in  the  Docker  container.
                 Each  key  and  value in the Environment string to string map
                 can have length of up to 1024. We support up to 16 entries in
                 the map.

                 key -> (string)

                 value -> (string)

              ModelPackageName -> (string)
                 The  name  or Amazon Resource Name (ARN) of the model package
                 to use to create the model.

              InferenceSpecificationName -> (string)
                 The inference specification name in the  model  package  ver-
                 sion.

              MultiModelConfig -> (structure)
                 Specifies additional configuration for multi-model endpoints.

                 ModelCacheSetting -> (string)
                     Whether to cache models for a  multi-model  endpoint.  By
                     default,  multi-model  endpoints  cache  models so that a
                     model does not have to be loaded into memory each time it
                     is  invoked.  Some  use  cases  do not benefit from model
                     caching. For example, if an endpoint hosts a large number
                     of  models  that  are each invoked infrequently, the end-
                     point might perform better if you disable model  caching.
                     To disable model caching, set the value of this parameter
                     to Disabled .

       Shorthand Syntax:

          ContainerHostname=string,Image=string,ImageConfig={RepositoryAccessMode=string,RepositoryAuthConfig={RepositoryCredentialsProviderArn=string}},Mode=string,ModelDataUrl=string,Environment={KeyName1=string,KeyName2=string},ModelPackageName=string,InferenceSpecificationName=string,MultiModelConfig={ModelCacheSetting=string} ...

       JSON Syntax:

          [
            {
              "ContainerHostname": "string",
              "Image": "string",
              "ImageConfig": {
                "RepositoryAccessMode": "Platform"|"Vpc",
                "RepositoryAuthConfig": {
                  "RepositoryCredentialsProviderArn": "string"
                }
              },
              "Mode": "SingleModel"|"MultiModel",
              "ModelDataUrl": "string",
              "Environment": {"string": "string"
                ...},
              "ModelPackageName": "string",
              "InferenceSpecificationName": "string",
              "MultiModelConfig": {
                "ModelCacheSetting": "Enabled"|"Disabled"
              }
            }
            ...
          ]

       --inference-execution-config (structure)
          Specifies details of how containers in  a  multi-container  endpoint
          are called.

          Mode -> (string)
              How  containers in a multi-container are run. The following val-
              ues are valid.

              o SERIAL - Containers run as a serial pipeline.

              o DIRECT - Only the individual container  that  you  specify  is
                run.

       Shorthand Syntax:

          Mode=string

       JSON Syntax:

          {
            "Mode": "Serial"|"Direct"
          }

       --execution-role-arn (string)
          The Amazon Resource Name (ARN) of the IAM role that Amazon SageMaker
          can assume to access model artifacts and docker image for deployment
          on ML compute instances or for batch transform jobs. Deploying on ML
          compute instances is part of model hosting.  For  more  information,
          see Amazon SageMaker Roles .

          NOTE:
              To  be able to pass this role to Amazon SageMaker, the caller of
              this API must have the iam:PassRole permission.

       --tags (list)
          An array of key-value pairs. You can use  tags  to  categorize  your
          Amazon  Web  Services  resources  in different ways, for example, by
          purpose, owner, or environment. For more  information,  see  Tagging
          Amazon Web Services Resources .

          (structure)
              A  tag object that consists of a key and an optional value, used
              to manage metadata for SageMaker Amazon Web Services  resources.

              You  can add tags to notebook instances, training jobs, hyperpa-
              rameter tuning jobs,  batch  transform  jobs,  models,  labeling
              jobs,  work  teams,  endpoint configurations, and endpoints. For
              more information on adding  tags  to  SageMaker  resources,  see
              AddTags .

              For  more information on adding metadata to your Amazon Web Ser-
              vices resources with tagging, see Tagging  Amazon  Web  Services
              resources . For advice on best practices for managing Amazon Web
              Services resources with tagging,  see  Tagging  Best  Practices:
              Implement  an  Effective  Amazon  Web  Services Resource Tagging
              Strategy .

              Key -> (string)
                 The tag key. Tag keys must be unique per resource.

              Value -> (string)
                 The tag value.

       Shorthand Syntax:

          Key=string,Value=string ...

       JSON Syntax:

          [
            {
              "Key": "string",
              "Value": "string"
            }
            ...
          ]

       --vpc-config (structure)
          A  VpcConfig object that specifies the VPC that you want your  model
          to  connect  to.  Control access to and from your model container by
          configuring the VPC. VpcConfig is used in hosting  services  and  in
          batch  transform.  For  more  information,  see Protect Endpoints by
          Using an Amazon Virtual Private Cloud  and  Protect  Data  in  Batch
          Transform Jobs by Using an Amazon Virtual Private Cloud .

          SecurityGroupIds -> (list)
              The VPC security group IDs, in the form sg-xxxxxxxx. Specify the
              security groups for the VPC that is  specified  in  the  Subnets
              field.

              (string)

          Subnets -> (list)
              The  ID  of  the subnets in the VPC to which you want to connect
              your training job or model. For information about the availabil-
              ity of specific instance types, see Supported Instance Types and
              Availability Zones .

              (string)

       Shorthand Syntax:

          SecurityGroupIds=string,string,Subnets=string,string

       JSON Syntax:

          {
            "SecurityGroupIds": ["string", ...],
            "Subnets": ["string", ...]
          }

       --enable-network-isolation | --no-enable-network-isolation (boolean)
          Isolates the model container. No inbound or outbound  network  calls
          can be made to or from the model container.

       --cli-input-json  |  --cli-input-yaml (string) Reads arguments from the
       JSON string provided. The JSON string follows the  format  provided  by
       --generate-cli-skeleton. If other arguments are provided on the command
       line, those values will override the JSON-provided values.  It  is  not
       possible to pass arbitrary binary values using a JSON-provided value as
       the string will be taken literally. This may  not  be  specified  along
       with --cli-input-yaml.

       --generate-cli-skeleton  (string)  Prints  a  JSON skeleton to standard
       output without sending an API request. If provided with no value or the
       value input, prints a sample input JSON that can be used as an argument
       for --cli-input-json. Similarly, if provided yaml-input it will print a
       sample  input  YAML that can be used with --cli-input-yaml. If provided
       with the value output, it validates the command inputs  and  returns  a
       sample output JSON for that command.

       See 'aws help' for descriptions of global parameters.

OUTPUT
       ModelArn -> (string)
          The ARN of the model created in Amazon SageMaker.



                                                                CREATE-MODEL()
