CREATE-JOB()                                                      CREATE-JOB()



NAME
       create-job -

DESCRIPTION
       Creates a new job definition.

       See also: AWS API Documentation

       See 'aws help' for descriptions of global parameters.

SYNOPSIS
            create-job
          --name <value>
          [--description <value>]
          [--log-uri <value>]
          --role <value>
          [--execution-property <value>]
          --command <value>
          [--default-arguments <value>]
          [--non-overridable-arguments <value>]
          [--connections <value>]
          [--max-retries <value>]
          [--allocated-capacity <value>]
          [--timeout <value>]
          [--max-capacity <value>]
          [--security-configuration <value>]
          [--tags <value>]
          [--notification-property <value>]
          [--glue-version <value>]
          [--number-of-workers <value>]
          [--worker-type <value>]
          [--cli-input-json | --cli-input-yaml]
          [--generate-cli-skeleton <value>]

OPTIONS
       --name (string)
          The  name  you  assign  to this job definition. It must be unique in
          your account.

       --description (string)
          Description of the job being defined.

       --log-uri (string)
          This field is reserved for future use.

       --role (string)
          The name or Amazon Resource Name (ARN) of the  IAM  role  associated
          with this job.

       --execution-property (structure)
          An  ExecutionProperty  specifying  the  maximum number of concurrent
          runs allowed for this job.

          MaxConcurrentRuns -> (integer)
              The maximum number of concurrent runs allowed for the  job.  The
              default  is  1.  An  error  is  returned  when this threshold is
              reached. The maximum value you can specify is  controlled  by  a
              service limit.

       Shorthand Syntax:

          MaxConcurrentRuns=integer

       JSON Syntax:

          {
            "MaxConcurrentRuns": integer
          }

       --command (structure)
          The JobCommand that runs this job.

          Name -> (string)
              The  name  of the job command. For an Apache Spark ETL job, this
              must be glueetl . For a Python shell job, it must be pythonshell
              .   For  an  Apache  Spark  streaming  ETL  job,  this  must  be
              gluestreaming .

          ScriptLocation -> (string)
              Specifies the Amazon Simple Storage Service (Amazon S3) path  to
              a script that runs a job.

          PythonVersion -> (string)
              The Python version being used to run a Python shell job. Allowed
              values are 2 or 3.

       Shorthand Syntax:

          Name=string,ScriptLocation=string,PythonVersion=string

       JSON Syntax:

          {
            "Name": "string",
            "ScriptLocation": "string",
            "PythonVersion": "string"
          }

       --default-arguments (map)
          The default arguments for this job.

          You can specify arguments here that your  own  job-execution  script
          consumes, as well as arguments that Glue itself consumes.

          For  information about how to specify and consume your own Job argu-
          ments, see the Calling Glue APIs in Python topic  in  the  developer
          guide.

          For  information about the key-value pairs that Glue consumes to set
          up your job, see the Special Parameters Used by Glue  topic  in  the
          developer guide.

          key -> (string)

          value -> (string)

       Shorthand Syntax:

          KeyName1=string,KeyName2=string

       JSON Syntax:

          {"string": "string"
            ...}

       --non-overridable-arguments (map)
          Non-overridable  arguments  for  this  job,  specified as name-value
          pairs.

          key -> (string)

          value -> (string)

       Shorthand Syntax:

          KeyName1=string,KeyName2=string

       JSON Syntax:

          {"string": "string"
            ...}

       --connections (structure)
          The connections used for this job.

          Connections -> (list)
              A list of connections used by the job.

              (string)

       Shorthand Syntax:

          Connections=string,string

       JSON Syntax:

          {
            "Connections": ["string", ...]
          }

       --max-retries (integer)
          The maximum number of times to retry this job if it fails.

       --allocated-capacity (integer)
          This parameter is deprecated. Use MaxCapacity instead.

          The number of Glue data processing units (DPUs) to allocate to  this
          Job.  You  can allocate from 2 to 100 DPUs; the default is 10. A DPU
          is a relative measure of processing power that consists of  4  vCPUs
          of  compute  capacity and 16 GB of memory. For more information, see
          the Glue pricing page .

       --timeout (integer)
          The job timeout in minutes. This is the maximum time that a job  run
          can  consume  resources  before  it is terminated and enters TIMEOUT
          status. The default is 2,880 minutes (48 hours).

       --max-capacity (double)
          For Glue version 1.0 or earlier  jobs,  using  the  standard  worker
          type,  the  number  of Glue data processing units (DPUs) that can be
          allocated when this job runs. A DPU is a relative  measure  of  pro-
          cessing power that consists of 4 vCPUs of compute capacity and 16 GB
          of memory. For more information, see the Glue pricing page .

          Do not set Max Capacity if using WorkerType and NumberOfWorkers .

          The value that can be allocated for MaxCapacity depends  on  whether
          you are running a Python shell job or an Apache Spark ETL job:

          o When  you  specify  a  Python shell job (JobCommand.Name ="python-
            shell"), you can allocate either 0.0625 or 1 DPU. The  default  is
            0.0625 DPU.

          o When   you  specify  an  Apache  Spark  ETL  job  (JobCommand.Name
            ="glueetl") or Apache Spark  streaming  ETL  job  (JobCommand.Name
            ="gluestreaming"),  you  can  allocate  from  2  to  100 DPUs. The
            default is 10 DPUs. This job type cannot  have  a  fractional  DPU
            allocation.

          For  Glue  version  2.0  jobs,  you cannot instead specify a Maximum
          capacity . Instead, you should specify a Worker type and the  Number
          of workers .

       --security-configuration (string)
          The name of the SecurityConfiguration structure to be used with this
          job.

       --tags (map)
          The tags to use with this job. You may use tags to limit  access  to
          the  job.  For  more  information about tags in Glue, see Amazon Web
          Services Tags in Glue in the developer guide.

          key -> (string)

          value -> (string)

       Shorthand Syntax:

          KeyName1=string,KeyName2=string

       JSON Syntax:

          {"string": "string"
            ...}

       --notification-property (structure)
          Specifies configuration properties of a job notification.

          NotifyDelayAfter -> (integer)
              After a job run starts, the number of  minutes  to  wait  before
              sending a job run delay notification.

       Shorthand Syntax:

          NotifyDelayAfter=integer

       JSON Syntax:

          {
            "NotifyDelayAfter": integer
          }

       --glue-version (string)
          Glue version determines the versions of Apache Spark and Python that
          Glue supports. The Python version indicates  the  version  supported
          for jobs of type Spark.

          For  more  information  about the available Glue versions and corre-
          sponding Spark and Python versions, see Glue version in  the  devel-
          oper guide.

          Jobs  that  are created without specifying a Glue version default to
          Glue 0.9.

       --number-of-workers (integer)
          The number of workers of a defined  workerType  that  are  allocated
          when a job runs.

          The  maximum number of workers you can define are 299 for G.1X , and
          149 for G.2X .

       --worker-type (string)
          The type of predefined worker that is allocated  when  a  job  runs.
          Accepts a value of Standard, G.1X, or G.2X.

          o For  the  Standard worker type, each worker provides 4 vCPU, 16 GB
            of memory and a 50GB disk, and 2 executors per worker.

          o For the G.1X worker type, each worker maps to 1 DPU (4 vCPU, 16 GB
            of  memory,  64  GB  disk), and provides 1 executor per worker. We
            recommend this worker type for memory-intensive jobs.

          o For the G.2X worker type, each worker maps to 2 DPU (8 vCPU, 32 GB
            of  memory,  128  GB disk), and provides 1 executor per worker. We
            recommend this worker type for memory-intensive jobs.

          Possible values:

          o Standard

          o G.1X

          o G.2X

       --cli-input-json | --cli-input-yaml (string) Reads arguments  from  the
       JSON  string  provided.  The JSON string follows the format provided by
       --generate-cli-skeleton. If other arguments are provided on the command
       line,  those  values  will override the JSON-provided values. It is not
       possible to pass arbitrary binary values using a JSON-provided value as
       the  string  will  be  taken literally. This may not be specified along
       with --cli-input-yaml.

       --generate-cli-skeleton (string) Prints a  JSON  skeleton  to  standard
       output without sending an API request. If provided with no value or the
       value input, prints a sample input JSON that can be used as an argument
       for --cli-input-json. Similarly, if provided yaml-input it will print a
       sample input YAML that can be used with --cli-input-yaml.  If  provided
       with  the  value  output, it validates the command inputs and returns a
       sample output JSON for that command.

       See 'aws help' for descriptions of global parameters.

EXAMPLES
       To create a job to transform data

       The following create-job example creates a streaming job  that  runs  a
       script stored in S3.

          aws glue create-job \
              --name my-testing-job \
              --role AWSGlueServiceRoleDefault \
              --command '{ \
                  "Name": "gluestreaming", \
                  "ScriptLocation": "s3://DOC-EXAMPLE-BUCKET/folder/" \
              }' \
              --region us-east-1 \
              --output json \
              --default-arguments '{ \
                  "--job-language":"scala", \
                  "--class":"GlueApp" \
              }' \
              --profile my-profile \
              --endpoint https://glue.us-east-1.amazonaws.com

       Contents of test_script.scala:

          import com.amazonaws.services.glue.ChoiceOption
          import com.amazonaws.services.glue.GlueContext
          import com.amazonaws.services.glue.MappingSpec
          import com.amazonaws.services.glue.ResolveSpec
          import com.amazonaws.services.glue.errors.CallSite
          import com.amazonaws.services.glue.util.GlueArgParser
          import com.amazonaws.services.glue.util.Job
          import com.amazonaws.services.glue.util.JsonOptions
          import org.apache.spark.SparkContext
          import scala.collection.JavaConverters._

          object GlueApp {
              def main(sysArgs: Array[String]) {
                  val spark: SparkContext = new SparkContext()
                  val glueContext: GlueContext = new GlueContext(spark)
                  // @params: [JOB_NAME]
                  val args = GlueArgParser.getResolvedOptions(sysArgs, Seq("JOB_NAME").toArray)
                  Job.init(args("JOB_NAME"), glueContext, args.asJava)
                  // @type: DataSource
                  // @args: [database = "tempdb", table_name = "s3-source", transformation_ctx = "datasource0"]
                  // @return: datasource0
                  // @inputs: []
                  val datasource0 = glueContext.getCatalogSource(database = "tempdb", tableName = "s3-source", redshiftTmpDir = "", transformationContext = "datasource0").getDynamicFrame()
                  // @type: ApplyMapping
                  // @args: [mapping = [("sensorid", "int", "sensorid", "int"), ("currenttemperature", "int", "currenttemperature", "int"), ("status", "string", "status", "string")], transformation_ctx = "applymapping1"]
                  // @return: applymapping1
                  // @inputs: [frame = datasource0]
                  val applymapping1 = datasource0.applyMapping(mappings = Seq(("sensorid", "int", "sensorid", "int"), ("currenttemperature", "int", "currenttemperature", "int"), ("status", "string", "status", "string")), caseSensitive = false, transformationContext = "applymapping1")
                  // @type: SelectFields
                  // @args: [paths = ["sensorid", "currenttemperature", "status"], transformation_ctx = "selectfields2"]
                  // @return: selectfields2
                  // @inputs: [frame = applymapping1]
                  val selectfields2 = applymapping1.selectFields(paths = Seq("sensorid", "currenttemperature", "status"), transformationContext = "selectfields2")
                  // @type: ResolveChoice
                  // @args: [choice = "MATCH_CATALOG", database = "tempdb", table_name = "my-s3-sink", transformation_ctx = "resolvechoice3"]
                  // @return: resolvechoice3
                  // @inputs: [frame = selectfields2]
                  val resolvechoice3 = selectfields2.resolveChoice(choiceOption = Some(ChoiceOption("MATCH_CATALOG")), database = Some("tempdb"), tableName = Some("my-s3-sink"), transformationContext = "resolvechoice3")
                  // @type: DataSink
                  // @args: [database = "tempdb", table_name = "my-s3-sink", transformation_ctx = "datasink4"]
                  // @return: datasink4
                  // @inputs: [frame = resolvechoice3]
                  val datasink4 = glueContext.getCatalogSink(database = "tempdb", tableName = "my-s3-sink", redshiftTmpDir = "", transformationContext = "datasink4").writeDynamicFrame(resolvechoice3)
                  Job.commit()
              }
          }

       Output:

          {
              "Name": "my-testing-job"
          }

       For  more  information,  see Authoring Jobs in AWS Glue in the AWS Glue
       Developer Guide.

OUTPUT
       Name -> (string)
          The unique name that was provided for this job definition.



                                                                  CREATE-JOB()
