GET-PERSON-TRACKING()                                    GET-PERSON-TRACKING()



NAME
       get-person-tracking -

DESCRIPTION
       Gets  the  path tracking results of a Amazon Rekognition Video analysis
       started by  StartPersonTracking .

       The person path tracking operation is started by a call to StartPerson-
       Tracking  which  returns  a job identifier (JobId ). When the operation
       finishes, Amazon Rekognition Video publishes a completion status to the
       Amazon Simple Notification Service topic registered in the initial call
       to StartPersonTracking .

       To get the results of the person path tracking operation,  first  check
       that  the status value published to the Amazon SNS topic is SUCCEEDED .
       If so, call  GetPersonTracking and pass the  job  identifier  (JobId  )
       from the initial call to StartPersonTracking .
          GetPersonTracking returns an array, Persons , of tracked persons and
          the time(s) their paths were tracked in the video.

       NOTE:
              GetPersonTracking only returns  the  default  facial  attributes
              (BoundingBox  ,  Confidence  , Landmarks , Pose , and Quality ).
              The other facial attributes listed in the  Face  object  of  the
              following response syntax are not returned.

          For  more  information,  see  FaceDetail  in  the Amazon Rekognition
          Developer Guide.

       By default, the array is sorted by  the  time(s)  a  person's  path  is
       tracked  in  the  video.  You can sort by tracked persons by specifying
       INDEX for the SortBy input parameter.

       Use the MaxResults parameter to limit the number of items returned.  If
       there  are  more  results  than  specified in MaxResults , the value of
       NextToken in the operation response contains  a  pagination  token  for
       getting  the next set of results. To get the next page of results, call
       GetPersonTracking and populate the NextToken request parameter with the
       token value returned from the previous call to GetPersonTracking .

       See also: AWS API Documentation

       See 'aws help' for descriptions of global parameters.

SYNOPSIS
            get-person-tracking
          --job-id <value>
          [--max-results <value>]
          [--next-token <value>]
          [--sort-by <value>]
          [--cli-input-json | --cli-input-yaml]
          [--generate-cli-skeleton <value>]

OPTIONS
       --job-id (string)
          The identifier for a job that tracks persons in a video. You get the
          JobId from a call to StartPersonTracking .

       --max-results (integer)
          Maximum number of results to return per paginated call. The  largest
          value  you  can specify is 1000. If you specify a value greater than
          1000, a maximum of 1000 results is returned. The  default  value  is
          1000.

       --next-token (string)
          If the previous response was incomplete (because there are more per-
          sons to retrieve), Amazon Rekognition  Video  returns  a  pagination
          token in the response. You can use this pagination token to retrieve
          the next set of persons.

       --sort-by (string)
          Sort to use for elements in the Persons array. Use TIMESTAMP to sort
          array  elements  by the time persons are detected. Use INDEX to sort
          by the tracked persons. If you sort by INDEX ,  the  array  elements
          for each person are sorted by detection confidence. The default sort
          is by TIMESTAMP .

          Possible values:

          o INDEX

          o TIMESTAMP

       --cli-input-json | --cli-input-yaml (string) Reads arguments  from  the
       JSON  string  provided.  The JSON string follows the format provided by
       --generate-cli-skeleton. If other arguments are provided on the command
       line,  those  values  will override the JSON-provided values. It is not
       possible to pass arbitrary binary values using a JSON-provided value as
       the  string  will  be  taken literally. This may not be specified along
       with --cli-input-yaml.

       --generate-cli-skeleton (string) Prints a  JSON  skeleton  to  standard
       output without sending an API request. If provided with no value or the
       value input, prints a sample input JSON that can be used as an argument
       for --cli-input-json. Similarly, if provided yaml-input it will print a
       sample input YAML that can be used with --cli-input-yaml.  If  provided
       with  the  value  output, it validates the command inputs and returns a
       sample output JSON for that command.

       See 'aws help' for descriptions of global parameters.

EXAMPLES
       To get the results of a people pathing operation

       The following get-person-tracking command displays  the  results  of  a
       people  pathing  operation  that  you  started  previously  by  calling
       start-person-tracking.

          aws rekognition get-person-tracking  \
              --job-id 1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef

       Output:

          {
              "Persons": [
                  {
                      "Timestamp": 500,
                      "Person": {
                          "BoundingBox": {
                              "Width": 0.4151041805744171,
                              "Top": 0.07870370149612427,
                              "Left": 0.0,
                              "Height": 0.9212962985038757
                          },
                          "Index": 0
                      }
                  },
                  {
                      "Timestamp": 567,
                      "Person": {
                          "BoundingBox": {
                              "Width": 0.4755208194255829,
                              "Top": 0.07777778059244156,
                              "Left": 0.0,
                              "Height": 0.9194444417953491
                          },
                          "Index": 0
                      }
                  }
              ],
              "NextToken": "D/vRIYNyhG79ugdta3f+8cRg9oSRo+HigGOuxRiYpTn0ExnqTi1CJektVAc4HrAXDv25eHYk",
              "JobStatus": "SUCCEEDED",
              "VideoMetadata": {
                  "Format": "QuickTime / MOV",
                  "FrameRate": 29.970617294311523,
                  "Codec": "h264",
                  "DurationMillis": 6806,
                  "FrameHeight": 1080,
                  "FrameWidth": 1920
              }
          }

       For more information, see People  Pathing  in  the  Amazon  Rekognition
       Developer Guide.

OUTPUT
       JobStatus -> (string)
          The current status of the person tracking job.

       StatusMessage -> (string)
          If  the  job  fails, StatusMessage provides a descriptive error mes-
          sage.

       VideoMetadata -> (structure)
          Information about a video that Amazon  Rekognition  Video  analyzed.
          Videometadata  is returned in every page of paginated responses from
          a Amazon Rekognition Video operation.

          Codec -> (string)
              Type of compression used in the analyzed video.

          DurationMillis -> (long)
              Length of the video in milliseconds.

          Format -> (string)
              Format of the analyzed video. Possible values are MP4,  MOV  and
              AVI.

          FrameRate -> (float)
              Number of frames per second in the video.

          FrameHeight -> (long)
              Vertical pixel dimension of the video.

          FrameWidth -> (long)
              Horizontal pixel dimension of the video.

          ColorRange -> (string)
              A  description  of  the  range  of  luminance values in a video,
              either LIMITED (16 to 235) or FULL (0 to 255).

       NextToken -> (string)
          If the response is truncated, Amazon Rekognition Video returns  this
          token  that  you  can  use in the subsequent request to retrieve the
          next set of persons.

       Persons -> (list)
          An array of the persons detected in the video and the time(s)  their
          path  was  tracked throughout the video. An array element will exist
          for each time a person's path is tracked.

          (structure)
              Details and path tracking information for a single time  a  per-
              son's  path is tracked in a video. Amazon Rekognition operations
              that track people's paths return  an  array  of  PersonDetection
              objects  with  elements for each time a person's path is tracked
              in a video.

              For more information, see GetPersonTracking in the Amazon Rekog-
              nition Developer Guide.

              Timestamp -> (long)
                 The  time,  in milliseconds from the start of the video, that
                 the person's path was tracked.

              Person -> (structure)
                 Details about a person whose path was tracked in a video.

                 Index -> (long)
                     Identifier for the person detected person within a video.
                     Use to keep track of the person throughout the video. The
                     identifier is not stored by Amazon Rekognition.

                 BoundingBox -> (structure)
                     Bounding box around the detected person.

                     Width -> (float)
                        Width of the bounding box as a ratio  of  the  overall
                        image width.

                     Height -> (float)
                        Height  of  the bounding box as a ratio of the overall
                        image height.

                     Left -> (float)
                        Left coordinate of the bounding  box  as  a  ratio  of
                        overall image width.

                     Top -> (float)
                        Top coordinate of the bounding box as a ratio of over-
                        all image height.

                 Face -> (structure)
                     Face details for the detected person.

                     BoundingBox -> (structure)
                        Bounding box of the face. Default attribute.

                        Width -> (float)
                            Width of the bounding box as a ratio of the  over-
                            all image width.

                        Height -> (float)
                            Height of the bounding box as a ratio of the over-
                            all image height.

                        Left -> (float)
                            Left coordinate of the bounding box as a ratio  of
                            overall image width.

                        Top -> (float)
                            Top  coordinate  of the bounding box as a ratio of
                            overall image height.

                     AgeRange -> (structure)
                        The estimated age range, in years, for the  face.  Low
                        represents  the  lowest  estimated age and High repre-
                        sents the highest estimated age.

                        Low -> (integer)
                            The lowest estimated age.

                        High -> (integer)
                            The highest estimated age.

                     Smile -> (structure)
                        Indicates whether or not the face is smiling, and  the
                        confidence level in the determination.

                        Value -> (boolean)
                            Boolean  value  that indicates whether the face is
                            smiling or not.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     Eyeglasses -> (structure)
                        Indicates whether or  not  the  face  is  wearing  eye
                        glasses,  and  the  confidence level in the determina-
                        tion.

                        Value -> (boolean)
                            Boolean value that indicates whether the  face  is
                            wearing eye glasses or not.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     Sunglasses -> (structure)
                        Indicates  whether  or  not  the  face is wearing sun-
                        glasses, and the confidence level  in  the  determina-
                        tion.

                        Value -> (boolean)
                            Boolean  value  that indicates whether the face is
                            wearing sunglasses or not.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     Gender -> (structure)
                        The predicted gender of a detected face.

                        Value -> (string)
                            The predicted gender of the face.

                        Confidence -> (float)
                            Level of confidence in the prediction.

                     Beard -> (structure)
                        Indicates whether or not the face has a beard, and the
                        confidence level in the determination.

                        Value -> (boolean)
                            Boolean  value that indicates whether the face has
                            beard or not.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     Mustache -> (structure)
                        Indicates whether or not the face has a mustache,  and
                        the confidence level in the determination.

                        Value -> (boolean)
                            Boolean  value that indicates whether the face has
                            mustache or not.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     EyesOpen -> (structure)
                        Indicates whether or not the  eyes  on  the  face  are
                        open, and the confidence level in the determination.

                        Value -> (boolean)
                            Boolean  value  that indicates whether the eyes on
                            the face are open.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     MouthOpen -> (structure)
                        Indicates whether or not the  mouth  on  the  face  is
                        open, and the confidence level in the determination.

                        Value -> (boolean)
                            Boolean  value that indicates whether the mouth on
                            the face is open or not.

                        Confidence -> (float)
                            Level of confidence in the determination.

                     Emotions -> (list)
                        The emotions that appear to be expressed on the  face,
                        and the confidence level in the determination. The API
                        is only making a determination of the physical appear-
                        ance  of a person's face. It is not a determination of
                        the persons internal emotional state and should not be
                        used  in  such a way. For example, a person pretending
                        to have a sad face might not be sad emotionally.

                        (structure)
                            The emotions that appear to be  expressed  on  the
                            face,  and  the confidence level in the determina-
                            tion. The API is only making  a  determination  of
                            the  physical appearance of a person's face. It is
                            not a determination of the persons  internal  emo-
                            tional state and should not be used in such a way.
                            For example, a person pretending  to  have  a  sad
                            face might not be sad emotionally.

                            Type -> (string)
                               Type of emotion detected.

                            Confidence -> (float)
                               Level of confidence in the determination.

                     Landmarks -> (list)
                        Indicates  the  location  of  landmarks  on  the face.
                        Default attribute.

                        (structure)
                            Indicates the location  of  the  landmark  on  the
                            face.

                            Type -> (string)
                               Type of landmark.

                            X -> (float)
                               The x-coordinate of the landmark expressed as a
                               ratio of the width of the image. The  x-coordi-
                               nate  is  measured  from  the  left-side of the
                               image. For example, if the image is 700  pixels
                               wide and the x-coordinate of the landmark is at
                               350 pixels, this value is 0.5.

                            Y -> (float)
                               The y-coordinate of the landmark expressed as a
                               ratio of the height of the image. The y-coordi-
                               nate is measured from the top of the image. For
                               example,  if the image height is 200 pixels and
                               the y-coordinate of the landmark is at 50  pix-
                               els, this value is 0.25.

                     Pose -> (structure)
                        Indicates  the  pose  of the face as determined by its
                        pitch, roll, and yaw. Default attribute.

                        Roll -> (float)
                            Value representing the face rotation on  the  roll
                            axis.

                        Yaw -> (float)
                            Value  representing  the  face rotation on the yaw
                            axis.

                        Pitch -> (float)
                            Value representing the face rotation on the  pitch
                            axis.

                     Quality -> (structure)
                        Identifies  image  brightness  and  sharpness. Default
                        attribute.

                        Brightness -> (float)
                            Value representing brightness  of  the  face.  The
                            service  returns a value between 0 and 100 (inclu-
                            sive). A higher value indicates  a  brighter  face
                            image.

                        Sharpness -> (float)
                            Value representing sharpness of the face. The ser-
                            vice returns a value between  0  and  100  (inclu-
                            sive).  A  higher  value  indicates a sharper face
                            image.

                     Confidence -> (float)
                        Confidence level that the bounding box contains a face
                        (and  not  a different object such as a tree). Default
                        attribute.



                                                         GET-PERSON-TRACKING()
